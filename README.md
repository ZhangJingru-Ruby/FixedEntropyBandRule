# ğŸ Fixed-Entropy Band Sampler

This repo hosts an *ultra-light* **replacement** for my neural-network **BandNet**.  
Instead of training, it **maps the normalised distanceâ€to-goal (Ï„)** directly to a softmax temperature and returns a probability over 10 radial bands:

Ï„ â†’ T(Ï„) â†’ softmax( âˆ’band_idx / T )

* Near goal (Ï„ â†’ 0) âœ tiny T âœ sharp focus on bands near the global path.  
* Far away (Ï„ â†’ 1) âœ large T âœ broad exploration across the corridor.

## ğŸŒŸ Why I chose this method

* **Zero training time** â€“ perfect baseline or quick deployment.
* **Guaranteed entropy control** â€“ no more over- or under-exploration.
* **One parameter** (`T_min`, `T_max`) to tune the spice level.

## ğŸ“‚ Project layout

```bash
.
â”œâ”€ run.py # main entry â€“ loops through all environments
â”œâ”€ utils/
â”‚ â”œâ”€ data_loader.py # unchanged fancy dataloader
â”‚ â”œâ”€ environment_functions.py
â”‚ â””â”€ New_Reward_Function.py
â””â”€ results/
â””â”€ data/ # I didn't upload it due to its size
```

## ğŸš€ Quick start

```bash
# clone & enter
git clone git@github.com:ZhangJingru-Ruby/FixedEntropyBandRule.git
cd FixedEntropyBandRule

# (optionally) activate your conda env
conda activate PY38-ZJR-subnn

# run!
python run.py
```

Outputs:
CSV log of reward + entropy per epoch.

Heat-map PNGs every 10th epoch (results/rule_*/rule_envX_epY.png)

## âš™ï¸ Key hyper-params

| Variable  | Default | Meaning                               |
| --------- | ------- | ------------------------------------- |
| `T_min`   | `0.05`  | Focused spread at Ï„=0                 |
| `T_max`   | `6.0`   | Exploratory spread at Ï„=1             |
| `ALPHA_R` | `5.0`   | Reward scale (same as BandNet papers) |



